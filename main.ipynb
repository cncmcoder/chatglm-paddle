{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T00:46:06.342382Z",
     "iopub.status.busy": "2023-05-17T00:46:06.341363Z",
     "iopub.status.idle": "2023-05-17T00:46:06.347095Z",
     "shell.execute_reply": "2023-05-17T00:46:06.346285Z",
     "shell.execute_reply.started": "2023-05-17T00:46:06.342335Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# ChatGLM-6B简介\n",
    "\n",
    "ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。\n",
    "\n",
    "具体参考ChatGLM-6B的GitHub\n",
    "https://github.com/THUDM/ChatGLM-6B\n",
    "\n",
    "\n",
    "# 1.项目介绍：\n",
    "\n",
    "为了方便大家学习、使用ChatGLM-6B。针对不同场景开发、测试ChatGLM 的功能。开发了多个函数:\n",
    "\n",
    "\n",
    "## 问答\n",
    "ask_glm(next_inputs,input_length,output_length)，这个函数可以一次性调用ChatGLM，虽然功能简单但是可以方便的与其他功能集成。适合大家利用开发自己的功能。\n",
    "\n",
    "运行效果如图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b8da9e50b5e447d29cbf2d08f2c3fd1c4757c5b8862347a6bb585c244d2b752a)\n",
    "\n",
    "\n",
    "## 多次交互\n",
    "chat_with_glm(input_prompt,input_length=2048,output_length=256)，则个函数可以与ChatGLM进行交谈，是命令行界面。\n",
    "运行效果如图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/4adf7a66145d40f3bac8fcff4c9dca3e36a6a32a4bb44766bd609f4b75ab3c12)\n",
    "\n",
    "\n",
    "## 图形界面，问答\n",
    "ask_glm_widgets(model,tokenizer,text='你好')，这个函数是使用ipywidgets界面与ChatGLM一次性交互。\n",
    "运行效果如图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7049dcdfc6524891979c9c51e17c58f6a774aa4b24b5401b955cebe22c0a8c95)\n",
    "\n",
    "\n",
    "## 图形界面，交互\n",
    "chat_glm_widgets(model,tokenizer)，这个函数是使用ipywidgets界面与ChatGLM交谈。\n",
    "运行效果如图：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0c816f57f96c43e0b752efe0e8b6a5e280165b64fb094e118cbe5cc0ab1b738a)\n",
    "\n",
    "\n",
    "\n",
    "分别用于一次性交流和多次交谈以及带图形界面的交流。\n",
    "\n",
    "注：\n",
    "\n",
    "编写调用GLM程序的时候借鉴了宇宙物语的 “基于ChatGLM-6B模型 + prompt实现角色扮演功能” 项目内容。 \n",
    "\n",
    "以及PaddleNLP的例子代码：\n",
    "https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/language_model/glm\n",
    "\n",
    "\n",
    "# 2.模型安装 （安装后重启内核）\n",
    "安装后点击重启内核。\n",
    "\n",
    "这部分内容只需要第一次运行的时候执行一遍，后续每次运行就不需要再执行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "print(paddle.version.cuda())\n",
    "!unzip paddlenlp.zip\n",
    "!cp -Rf paddlenlp /home/aistudio/.data/webide/pip/lib/python3.7/site-packages/paddlenlp\n",
    "!python -m pip install paddlepaddle-gpu==0.0.0.post112 -f https://www.paddlepaddle.org.cn/whl/linux/gpu/develop.html --user\n",
    "#  注意安装后重启内核"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.模型调用\n",
    "\n",
    "封装了4个函数:\n",
    "\n",
    "1，ask_glm(next_inputs,input_length,output_length)\n",
    "\n",
    "直接调用ChatGLM模型，输入参数：初始prompt, 最长输入长度，最长输出长度\n",
    "\n",
    "\n",
    "\n",
    "2，chat_with_glm(input_prompt,input_length=2048,output_length=256)\n",
    "\n",
    "与ChatGLM多次交互 输入参数：初始prompt, 最长输入长度，最长输出长度。交谈时，输入每一句后点击回车即可。需要退出时，输入：'1','退出','exit','bye','结束'都可以\n",
    "\n",
    "\n",
    "3，ask_glm_widgets(model,tokenizer,text='你好')\n",
    "\n",
    "ipywidgets图形界面交互，与ChatGLM交流，不展示历史消息\n",
    "\n",
    "4，chat_glm_widgets(model,tokenizer)\n",
    "\n",
    "ipywidgets图形界面 与ChatGLM交流，发送及展示历史消息\n",
    "\n",
    "\n",
    "\n",
    "分别用于一次性交流和多次交谈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-07-14T02:37:35.857657Z",
     "iopub.status.busy": "2023-07-14T02:37:35.857108Z",
     "iopub.status.idle": "2023-07-14T02:37:38.159792Z",
     "shell.execute_reply": "2023-07-14T02:37:38.159033Z",
     "shell.execute_reply.started": "2023-07-14T02:37:35.857627Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "import paddle\n",
    "import os\n",
    "from IPython.display import clear_output as clear\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_model(local_model_path):\n",
    "    from paddlenlp.transformers import (\n",
    "        ChatGLMConfig,\n",
    "        ChatGLMForConditionalGeneration,\n",
    "        ChatGLMTokenizer,\n",
    "    )\n",
    "    #读取原始的chatglm-6b模型\n",
    "    model_name_or_path = local_model_path\n",
    "    \n",
    "    tokenizer = ChatGLMTokenizer.from_pretrained(model_name_or_path)\n",
    "    config = ChatGLMConfig.from_pretrained(model_name_or_path)\n",
    "    paddle.set_default_dtype(config.paddle_dtype)\n",
    "\n",
    "    model = ChatGLMForConditionalGeneration.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        tensor_parallel_degree=paddle.distributed.get_world_size(),\n",
    "        tensor_parallel_rank=0,\n",
    "        load_state_as_np=True,\n",
    "        dtype=config.paddle_dtype,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    return model,tokenizer\n",
    "\n",
    "def convert_example(example, tokenizer, is_test=True):\n",
    "    query = example[\"user_input\"]\n",
    "    response = example[\"sys_output\"]\n",
    "    history = example.get(\"history\", None)\n",
    "    if history is None or len(history) == 0:\n",
    "        prompt = query\n",
    "    else:\n",
    "        prompt = \"\"\n",
    "        for i, (old_query, old_response) in enumerate(history):\n",
    "            prompt += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(i, old_query, old_response)\n",
    "        prompt += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n",
    "    return prompt\n",
    "\n",
    "# 问glm问题\n",
    "# 输入参数：初始prompt, 最长输入长度，最长输出长度\n",
    "def ask_glm(model,tokenizer,next_inputs,input_length,output_length):\n",
    "    inputs = tokenizer(\n",
    "        next_inputs,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "        max_length=input_length,\n",
    "        truncation=True,\n",
    "        truncation_side=\"left\",\n",
    "    )\n",
    "    input_map = {}\n",
    "    for key in inputs:\n",
    "        input_map[key] = paddle.to_tensor(inputs[key])\n",
    "\n",
    "    infer_result = model.generate(\n",
    "        **input_map,\n",
    "        decode_strategy=\"sampling\",\n",
    "        top_k=1,\n",
    "        # top_p =5,\n",
    "        max_length=output_length,\n",
    "        use_cache=True,\n",
    "        use_fast=True,\n",
    "        use_fp16_decoding=True,\n",
    "        repetition_penalty=1,\n",
    "        temperature = 0.95,\n",
    "        length_penalty=1,\n",
    "    )[0]\n",
    "    output = ''\n",
    "    result = []\n",
    "    for x in infer_result.tolist():\n",
    "        res = tokenizer.decode(x, skip_special_tokens=True)\n",
    "        res = res.strip(\"\\n\")\n",
    "        result.append(res)\n",
    "        print(res)\n",
    "        output = output + res\n",
    "    return output\n",
    "\n",
    "# 与GLM交谈\n",
    "# 输入参数：初始prompt, 最长输入长度，最长输出长度\n",
    "# 交谈时，输入每一句后点击回车即可\n",
    "# 需要退出时，输入：'1','退出','exit','bye','结束'都可以\n",
    "def chat_with_glm(model,tokenizer,input_prompt,input_length=2048,output_length=256):\n",
    "    start = 1\n",
    "\n",
    "    while(1):\n",
    "        if(start):\n",
    "            prompt = input_prompt\n",
    "            example = {'user_input':'' , 'sys_output':'' ,'history':[]}\n",
    "            user_input = prompt\n",
    "            print('问：'+prompt)\n",
    "            print('答：',end='')\n",
    "            start = 0\n",
    "        else:\n",
    "            user_input = input()\n",
    "            if(user_input in ['1','退出','exit','bye','结束','再见']):\n",
    "                print('退出')\n",
    "                break\n",
    "            os.system('cls' if os.name == 'nt' else 'clear')\n",
    "            clear()\n",
    "            print(user_input)\n",
    "            example['user_input'] = user_input\n",
    "            prompt = convert_example(example, tokenizer, is_test=True)\n",
    "            if(len(prompt)>input_length):\n",
    "                prompt = prompt[-input_length:-1]\n",
    "            print(prompt,end='')\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"np\",\n",
    "            padding=True,\n",
    "            max_length=input_length,\n",
    "            truncation=True,\n",
    "            truncation_side=\"left\",\n",
    "        )\n",
    "        input_map = {}\n",
    "        for key in inputs:\n",
    "            input_map[key] = paddle.to_tensor(inputs[key])\n",
    "\n",
    "        infer_result = model.generate(\n",
    "            **input_map,\n",
    "            decode_strategy=\"sampling\",\n",
    "            top_k=1,\n",
    "            max_length=output_length,\n",
    "            use_cache=True,\n",
    "            use_fast=True,\n",
    "            use_fp16_decoding=True,\n",
    "            repetition_penalty=1,\n",
    "            temperature = 0.95,\n",
    "            length_penalty=1,\n",
    "        )[0]\n",
    "        res = tokenizer.decode(infer_result.tolist()[0], skip_special_tokens=True)\n",
    "        res = res.strip(\"\\n\")\n",
    "        example['sys_output'] = res\n",
    "        example['history'].append((user_input,res))\n",
    "        print(res)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "# 图形界面 与ChatGLM交流，不展示历史消息\n",
    "def ask_glm_widgets(model,tokenizer,text='你好'):\n",
    "    # 创建两个Textarea小部件\n",
    "    txtprompt = widgets.Textarea(description='提示:',rows=15,layout=widgets.Layout(width='48%'))\n",
    "    txtresult = widgets.Textarea(description='结果:',rows=15,layout=widgets.Layout(width='48%'))\n",
    "\n",
    "    txtprompt.value=text\n",
    "\n",
    "    # 创建一个按钮小部件\n",
    "    btngo = widgets.Button(description='生成',layout=widgets.Layout(width='50px'), button_style='info')\n",
    "\n",
    "    # 定义按钮的点击事件处理程序\n",
    "    def send_prompt(btn):\n",
    "        result= ask_glm(model,tokenizer,txtprompt.value,2048,2048)\n",
    "        txtresult.value = result\n",
    "\n",
    "    # 绑定按钮的点击事件处理程序\n",
    "    btngo.on_click(send_prompt)\n",
    "\n",
    "    # 创建一个布局盒子，使用Flexbox布局\n",
    "    box_layout = widgets.Layout(display='flex', flex_flow='row', align_items='center')\n",
    "\n",
    "    # 将txtprompt、btngo和txtresult放入布局盒子中\n",
    "    hbox = widgets.HBox([txtprompt, btngo, txtresult], layout=box_layout)\n",
    "\n",
    "    # 显示布局盒子\n",
    "    display(hbox)\n",
    "\n",
    "# 图形界面 与ChatGLM交流，发送及展示历史消息\n",
    "def chat_glm_widgets(model,tokenizer):\n",
    "    # 创建一个Textarea小部件用于输入聊天消息\n",
    "    # txtinput = widgets.Text(description='输入:', layout=widgets.Layout(width='95%'))\n",
    "    txtinput = widgets.Textarea(description='输入:', layout=widgets.Layout(width='95%', height='150px'))\n",
    "\n",
    "    # 创建一个Textarea小部件用于显示聊天历史\n",
    "    txthistory = widgets.Textarea(description='历史:', layout=widgets.Layout(width='95%', height='200px'), disabled=True)\n",
    "\n",
    "    # 创建一个按钮小部件用于发送消息\n",
    "    btnsend = widgets.Button(description='发送', button_style='info')\n",
    "\n",
    "    # 创建一个按钮小部件用于重置历史\n",
    "    btnreset = widgets.Button(description='重置', button_style='info')\n",
    "\n",
    "    # 定义发送消息的函数\n",
    "    def send_message(_):\n",
    "        # 获取用户输入的消息\n",
    "        message = txtinput.value.strip()\n",
    "\n",
    "        if message:\n",
    "            # 添加用户输入的消息到聊天历史\n",
    "            txthistory.value += f'问: {message}\\n'\n",
    "\n",
    "            # 调用本地部署的LLM并获取回答结果\n",
    "            #result = ask_glm(message)\n",
    "            #result = ask_glm(model,tokenizer,message,2048,2048)\n",
    "            result = ask_glm(model,tokenizer,txthistory.value,2048,2048)\n",
    "\n",
    "            # 添加LLM的回答到聊天历史\n",
    "            txthistory.value += f'答: {result}\\n\\n'\n",
    "\n",
    "            # 清空输入框\n",
    "            txtinput.value = ''\n",
    "\n",
    "    # 绑定发送消息按钮的点击事件处理程序\n",
    "    btnsend.on_click(send_message)\n",
    "\n",
    "    # 定义重置历史的函数\n",
    "    def reset_history(_):\n",
    "        # 清空聊天历史和输入框\n",
    "        txthistory.value = ''\n",
    "        txtinput.value = ''\n",
    "\n",
    "    # 绑定重置历史按钮的点击事件处理程序\n",
    "    btnreset.on_click(reset_history)\n",
    "\n",
    "    # 将发送消息按钮和重置历史按钮放入水平布局盒子\n",
    "    buttons_box = widgets.HBox([btnsend, btnreset], layout=widgets.Layout(justify_content='center'))\n",
    "\n",
    "    # 创建一个垂直布局盒子，将小部件放入其中\n",
    "    vbox = widgets.VBox([txtinput, buttons_box, txthistory])\n",
    "\n",
    "\n",
    "    # 显示布局盒子\n",
    "    display(vbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.模型效果测试\n",
    "\n",
    "针对不同场景进行测试，整体感觉效果是不错的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 读取模型\r\n",
    "model,tokenizer = get_model('data/data217141')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T00:36:40.668801Z",
     "iopub.status.busy": "2023-05-30T00:36:40.667314Z",
     "iopub.status.idle": "2023-05-30T00:36:41.392264Z",
     "shell.execute_reply": "2023-05-30T00:36:41.391176Z",
     "shell.execute_reply.started": "2023-05-30T00:36:40.668753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京。\r\n"
     ]
    }
   ],
   "source": [
    "result=ask_glm(model,tokenizer,'中国的首都是哪个城市？',2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T00:51:45.648402Z",
     "iopub.status.busy": "2023-05-17T00:51:45.647721Z",
     "iopub.status.idle": "2023-05-17T00:52:10.141661Z",
     "shell.execute_reply": "2023-05-17T00:52:10.140907Z",
     "shell.execute_reply.started": "2023-05-17T00:51:45.648364Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python是一种高级编程语言,具有简单易学、可读性强、功能强大、可移植性好等特点。Python最初由Guido van Rossum在1989年开发,并于1991年首次发布。\r\n",
      "\r\n",
      "Python语言具有以下特点:\r\n",
      "\r\n",
      "1. 简单易学:Python语法简洁明了,易于理解和学习,不需要掌握复杂的编程概念和技巧。\r\n",
      "\r\n",
      "2. 可读性强:Python采用缩进作为代码标识符,使得代码可读性很强,同时也方便代码维护。\r\n",
      "\r\n",
      "3. 功能强大:Python拥有大量的内置模块,可以用于各种编程任务,如数据处理、网络编程、Web开发、人工智能等。\r\n",
      "\r\n",
      "4. 可移植性好:Python语言可以在不同的操作系统和硬件平台上运行,不需要进行特定的编译或改写。\r\n",
      "\r\n",
      "5. 跨平台:Python可以在Windows、MacOS、Linux等多个操作系统上运行,同时也支持多种编程环境,如IDLE、PyCharm等。\r\n",
      "\r\n",
      "Python语言广泛应用于各个领域,如科学计算、数据分析、人工智能、机器学习、自然语言处理、Web开发等。同时,Python语言也是开源社区的重要成员,有大量的开源库和框架可供使用。\r\n"
     ]
    }
   ],
   "source": [
    "result=ask_glm(model,tokenizer,'介绍一下Python语言',2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T00:53:58.852693Z",
     "iopub.status.busy": "2023-05-17T00:53:58.852080Z",
     "iopub.status.idle": "2023-05-17T00:55:45.757497Z",
     "shell.execute_reply": "2023-05-17T00:55:45.756592Z",
     "shell.execute_reply.started": "2023-05-17T00:53:58.852657Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们设计一个面向儿童的关于友情的插画书，5页左右，请设计每页的内容。\r\n",
      "[Round 0]\r\n",
      "问：你是一个作者，我是一个编辑，我们一起设计一本图书\r\n",
      "答：很高兴能和您一起设计一本图书!作为一个作者,我希望您能告诉我关于这本书的一些想法和计划,包括主题、情节、角色、风格和其他方面。作为一个编辑,我的职责是确保这本书的质量和准确性,并与您一起制定出版计划。我们可以一起讨论并制定一个详细的计划,以确保这本书能够达到预期的效果。\r\n",
      "[Round 1]\r\n",
      "问：我们来编辑一本儿童书吧\r\n",
      "答：好的,那我们可以开始讨论了!首先,我们可以讨论一下您想要的主题、情节、角色和风格,并确定一些基本的想法。然后,我们可以进一步讨论细节,例如用词、句式、插图和排版等方面,以确保这本书的整体质量。最后,我们可以制定出版计划,并确定出版日期和预算。让我们一起开始吧!\r\n",
      "[Round 2]\r\n",
      "问：我们设计一个面向儿童的关于友情的插画书，5页左右，请设计每页的内容。\r\n",
      "答：好的,让我们开始设计这本书吧!\r\n",
      "\r\n",
      "第1页:这是一个关于友情的故事,告诉我们友谊的重要性。插图展示了两个好朋友在一起玩耍的场景,他们互相支持、帮助和尊重。文字告诉我们,友谊是一种无私的奉献,可以带来快乐和幸福。\r\n",
      "\r\n",
      "第2页:在这个页中,我们继续介绍友谊的重要性,并讨论如何建立和维护好朋友关系。插图展示了两个好朋友在一起学习的场景,他们互相鼓励、帮助和支持,形成了良好的学习氛围。文字告诉我们,友谊是一种互相支持和帮助的关系,可以帮助人们克服困难,取得成功。\r\n",
      "\r\n",
      "第3页:在这个页中,我们讨论友谊的另一种形式,即团队友谊。插图展示了两个好朋友在一起工作的场景,他们互相支持、帮助和尊重,形成了良好的团队氛围。文字告诉我们,友谊不仅是一种个人关系,也是一种团队关系,可以帮助人们更好地合作和完成任务。\r\n",
      "\r\n",
      "第4页:在这个页中,我们讨论友谊的负面影响,即嫉妒和争吵。插图展示了两个好朋友之间发生争吵的场景,他们互相指责和批评,形成了不良的友谊关系。文字告诉我们,嫉妒和争吵是友谊中常见的问题,但如果不及时解决,会对友谊造成负面影响。\r\n",
      "\r\n",
      "第5页:在这个页中,我们总结友谊的重要性,并鼓励孩子们珍惜友谊。插图展示了两个好朋友在一起庆祝的场景,他们互相拥抱和祝福,形成了良好的友谊关系。文字告诉我们,友谊是一种珍贵的财富,可以带来快乐和幸福,我们应该珍惜友谊,并与他人建立良好的关系。\r\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bye\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "退出\r\n"
     ]
    }
   ],
   "source": [
    "chat_with_glm(model,tokenizer,'你是一个作者，我是一个编辑，我们一起设计一本图书',2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T00:56:21.475210Z",
     "iopub.status.busy": "2023-05-17T00:56:21.474535Z",
     "iopub.status.idle": "2023-05-17T00:57:47.634124Z",
     "shell.execute_reply": "2023-05-17T00:57:47.633090Z",
     "shell.execute_reply.started": "2023-05-17T00:56:21.475164Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "算错了\r\n",
      "[Round 0]\r\n",
      "问：你是一个数学老师，我是一个学生，我们一起在作算术题\r\n",
      "答：好的,我很高兴能和您一起解决算术问题。请问您有什么问题或需要我的帮助吗?\r\n",
      "[Round 1]\r\n",
      "问：5x=12 x是多少？\r\n",
      "答：将等式两边同时除以5,可以得到:\r\n",
      "\r\n",
      "x = 12 / 5\r\n",
      "\r\n",
      "将分数进行运算,得到:\r\n",
      "\r\n",
      "x = 2.4\r\n",
      "\r\n",
      "因此,x的值为2.4。\r\n",
      "[Round 2]\r\n",
      "问：5*8+9=？\r\n",
      "答：5*8+9=43\r\n",
      "[Round 3]\r\n",
      "问：算错了\r\n",
      "答：非常抱歉,我的回答有误。正确的答案应该是:\r\n",
      "\r\n",
      "5*8+9=43\r\n",
      "\r\n",
      "将等式两边同时乘以5,得到:\r\n",
      "\r\n",
      "5*8+9=43\r\n",
      "\r\n",
      "因此,5*8+9的值为43。\r\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bye\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "退出\r\n"
     ]
    }
   ],
   "source": [
    "chat_with_glm(model,tokenizer,'你是一个数学老师，我是一个学生，我们一起在作算术题',2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T02:39:58.276361Z",
     "iopub.status.busy": "2023-07-14T02:39:58.275570Z",
     "iopub.status.idle": "2023-07-14T02:39:58.298130Z",
     "shell.execute_reply": "2023-07-14T02:39:58.297601Z",
     "shell.execute_reply.started": "2023-07-14T02:39:58.276327Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d8bb02e20d4cb4a54ecd38269922cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='你好', description='提示:', layout=Layout(width='48%'), rows=15), Button(button_sty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_glm_widgets(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T02:40:46.347731Z",
     "iopub.status.busy": "2023-07-14T02:40:46.347138Z",
     "iopub.status.idle": "2023-07-14T02:40:46.379545Z",
     "shell.execute_reply": "2023-07-14T02:40:46.378825Z",
     "shell.execute_reply.started": "2023-07-14T02:40:46.347696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0457b94ae0ce4e3db988afe388cbeea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='输入:', layout=Layout(height='150px', width='95%')), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_glm_widgets(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
